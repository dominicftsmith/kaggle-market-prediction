{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Market Beater\n",
        "#### Kaggle's Hull Tactical - Market Prediction Competition\n",
        "---\n",
        "\n",
        "**Predicting Market Predictablity**\n",
        "\n",
        "**By:** Dominic Smith (for I Understand AI)"
      ],
      "metadata": {
        "id": "m_iMHAaGpAmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "bLemCYwAcx5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LOAD DATA"
      ],
      "metadata": {
        "id": "Y4hHa5ckTGDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# Defining path to Google Drive data\n",
        "FILE_PATH = '/content/drive/MyDrive/Kaggle Beat S&P'\n",
        "\n",
        "# --- Loading S&P 500 Daily Price Data ---\n",
        "try:\n",
        "    sp500_df = pd.read_csv(FILE_PATH + '/SP500_daily.csv')\n",
        "    print(\"S&P 500 data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"S&P 500 file not found. Creating a placeholder DataFrame for demonstration.\")\n",
        "    date_range = pd.date_range(start='2015-01-01', end='2025-01-01', freq='B') # Business days\n",
        "    placeholder_prices = 1500 + np.random.randn(len(date_range)).cumsum() * 5\n",
        "    sp500_df = pd.DataFrame({'Date': date_range, 'Close': placeholder_prices})\n",
        "    # Simulate holiday gaps\n",
        "    holidays = np.random.choice(sp500_df.index, 20, replace=False)\n",
        "    sp500_df.loc[holidays, 'Close'] = np.nan\n",
        "    print(\"Placeholder data created with simulated holiday gaps.\")\n",
        "\n",
        "# --- Load Politician Trading Data ---\n",
        "pol_df = pd.read_csv(FILE_PATH + '/insider1.csv')\n",
        "print(\"Politician trading data loaded successfully.\")"
      ],
      "metadata": {
        "id": "jzxl7SjnExTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA PREPROCESSING AND CLEANING"
      ],
      "metadata": {
        "id": "rdLi22e3S1b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load S&P 500 Price Data ---\n",
        "\n",
        "try:\n",
        "    # Correctly load 'SP500_daily.csv' file\n",
        "    # and tell pandas to treat 'observation_date' as a date while loading\n",
        "    sp500_df = pd.read_csv(\n",
        "        FILE_PATH + '/SP500_daily.csv',\n",
        "        parse_dates=['observation_date']\n",
        "    )\n",
        "\n",
        "    # Rename the columns to 'Date' and 'Close' so all the later code works\n",
        "    sp500_df = sp500_df.rename(columns={\n",
        "        'observation_date': 'Date',\n",
        "        'SP500': 'Close'\n",
        "    })\n",
        "\n",
        "    print(\"Successfully loaded SP500_daily.csv and renamed columns.\")\n",
        "    print(\"DataFrame head:\")\n",
        "    print(sp500_df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Could not find 'SP500_daily.csv' in your Google Drive folder.\")\n",
        "    print(\"Please double-check the filename and folder location.\")\n",
        "    raise"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dcuyGqgSruRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. LOAD AND CLEAN POLITICIAN DATA (Revised with Explicit Type Conversion) ---\n",
        "print(\"\\n--- Step 2: Loading and Cleaning Politician Data ---\")\n",
        "try:\n",
        "    pol_df = pd.read_csv(FILE_PATH + '/insider1.csv')\n",
        "\n",
        "    # --- Convert 'Traded' column to datetime ---\n",
        "    # Clean the string first, then convert.\n",
        "    pol_df['Traded'] = pd.to_datetime(pol_df['Traded'].str.replace('\\n', ' '), format='mixed', errors='coerce')\n",
        "\n",
        "    # Add this line to clean the 'Published' date column\n",
        "    pol_df['Published'] = pd.to_datetime(pol_df['Published'].str.replace('\\n', ' '), format='mixed', errors='coerce')\n",
        "\n",
        "    # --- Convert 'Price' column to numeric ---\n",
        "    # The 'Price' column may have '$' signs. This will remove them and convert to a number.\n",
        "    # 'coerce' will turn any errors (like 'N/A' text) into NaN (Not a Number).\n",
        "    pol_df['Price'] = pd.to_numeric(pol_df['Price'].astype(str).str.replace('$', ''), errors='coerce')\n",
        "\n",
        "    # --- Convert 'Size' column to a numeric estimate ---\n",
        "    # This function handles ranges like '1K–15K'.\n",
        "    def size_to_numeric(size_range):\n",
        "        if isinstance(size_range, str):\n",
        "            size_range = size_range.replace('K', '000').replace('M', '000000').replace('$', '')\n",
        "            parts = size_range.split('–')\n",
        "            if len(parts) == 2:\n",
        "                try: return (float(parts[0]) + float(parts[1])) / 2\n",
        "                except ValueError: return np.nan\n",
        "        return np.nan\n",
        "\n",
        "    pol_df['EstimatedValue'] = pol_df['Size'].apply(size_to_numeric)\n",
        "\n",
        "    # Drop rows that couldn't be converted properly\n",
        "    pol_df = pol_df.dropna(subset=['Traded', 'EstimatedValue', 'Price'])\n",
        "\n",
        "    print(\"Politician data loaded and all relevant columns converted successfully.\")\n",
        "    print(\"\\nCleaned pol_df dtypes:\")\n",
        "    print(pol_df[['Traded', 'Price', 'EstimatedValue']].info())\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Could not find the insider1.csv file.\") # More specific error message\n",
        "    raise"
      ],
      "metadata": {
        "id": "iGfaaig-oaS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Process S&P 500 Data ---\n",
        "# The 'observation_date' column is already the datetime index, so no need to convert or set index.\n",
        "\n",
        "# Use forward-fill to handle holidays that might be present in the original data\n",
        "sp500_df['Close'] = sp500_df['Close'].ffill()\n",
        "print(\"\\nHandled missing holiday data in original S&P 500 data using forward-fill.\")\n",
        "\n",
        "# --- Process Politician Trading Data ---\n",
        "# Convert 'Traded' date column to datetime, coercing errors to NaT\n",
        "pol_df['Traded'] = pd.to_datetime(pol_df['Traded'], errors='coerce')\n",
        "\n",
        "# Function to convert trade size ranges (e.g., '1K–15K') to a numeric estimate\n",
        "def size_to_numeric(size_range):\n",
        "    if isinstance(size_range, str):\n",
        "        size_range = size_range.replace('K', '000').replace('M', '000000').replace('$', '')\n",
        "        parts = size_range.split('–')\n",
        "        if len(parts) == 2:\n",
        "            try:\n",
        "                return (float(parts[0]) + float(parts[1])) / 2\n",
        "            except ValueError:\n",
        "                return np.nan\n",
        "    return np.nan\n",
        "\n",
        "pol_df['EstimatedValue'] = pol_df['Size'].apply(size_to_numeric)\n",
        "pol_df = pol_df.dropna(subset=['EstimatedValue'])"
      ],
      "metadata": {
        "id": "fH3cvdxiSwt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "NHgOS9z10fg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with our clean S&P 500 data as the base\n",
        "main_df = sp500_df.copy()\n",
        "\n",
        "# Set the 'Date' column as the index and drop the original 'Date' column\n",
        "main_df = main_df.set_index('Date')\n",
        "\n",
        "\n",
        "# --- 1. Price-Based Features ---\n",
        "# These are calculated first, using only the S&P 500 data\n",
        "main_df['Volatility_20D'] = main_df['Close'].rolling(window=20).std()\n",
        "main_df['Momentum_20D'] = main_df['Close'] - main_df['Close'].shift(20)\n",
        "main_df['SMA_50'] = main_df['Close'].rolling(window=50).mean()\n",
        "\n",
        "# --- 2. Political Trading Features ---\n",
        "# Create a daily summary of political trades\n",
        "# Ensure the index is a datetime type for proper merging\n",
        "daily_trades = pol_df.groupby(pol_df['Traded'].dt.date).agg(\n",
        "    buy_count=('Type', lambda x: (x == 'buy').sum()),\n",
        "    sell_count=('Type', lambda x: (x == 'sell').sum()),\n",
        "    buy_volume=('EstimatedValue', lambda x: x[pol_df['Type'] == 'buy'].sum()),\n",
        "    sell_volume=('EstimatedValue', lambda x: x[pol_df['Type'] == 'sell'].sum())\n",
        ")\n",
        "daily_trades.index = pd.to_datetime(daily_trades.index) # Convert index to datetime\n",
        "\n",
        "# Calculate net metrics\n",
        "daily_trades['net_trades'] = daily_trades['buy_count'] - daily_trades['sell_count']\n",
        "daily_trades['net_volume'] = daily_trades['buy_volume'] - daily_trades['sell_volume']\n",
        "\n",
        "# --- 3. Merge Datasets ---\n",
        "# Merge the political features into our main DataFrame.\n",
        "# Use a 'left' merge to keep all the S&P 500 dates.\n",
        "# Days with no political trades will have NaN values.\n",
        "main_df = main_df.merge(\n",
        "    daily_trades[['net_trades', 'net_volume']],\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill the NaN values for days with no trades with 0\n",
        "main_df[['net_trades', 'net_volume']] = main_df[['net_trades', 'net_volume']].fillna(0)\n",
        "\n",
        "# Reindex to a full daily calendar to ensure all weekdays are present *after* merging\n",
        "# This step introduced NaT values, removing it.\n",
        "# main_df = main_df.asfreq('D')\n",
        "\n",
        "# Use forward-fill again to handle weekends and holidays introduced by asfreq\n",
        "# main_df['Close'] = main_df['Close'].ffill()\n",
        "\n",
        "\n",
        "# --- 4. Define the Target Variable ---\n",
        "# The target is the 5-day future return. This must be the LAST step.\n",
        "main_df['target'] = (main_df['Close'].shift(-5) - main_df['Close']) / main_df['Close']\n",
        "\n",
        "print(\"Feature engineering and merge complete.\")\n",
        "print(\"DataFrame tail (showing recent data with political trades):\")\n",
        "print(main_df.tail(10))"
      ],
      "metadata": {
        "id": "todZGbTn0dG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MODEL TRAINING"
      ],
      "metadata": {
        "id": "Z4iE-TXy3KCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5. MODEL TRAINING (with Hyperparameter Tuning v1)\n",
        "# =============================================================================\n",
        "\n",
        "# --- Prepare Final DataFrame ---\n",
        "final_df = main_df.dropna()\n",
        "print(f\"Number of rows after dropna(): {len(final_df)}\")\n",
        "\n",
        "# --- Define Features (X) and Target (y) ---\n",
        "# Using the original, best-performing feature set\n",
        "features = ['Volatility_20D', 'Momentum_20D', 'SMA_50', 'net_trades', 'net_volume']\n",
        "X = final_df[features]\n",
        "y = final_df['target']\n",
        "\n",
        "# --- Split Data Using a Specific Date ---\n",
        "split_date = '2023-03-14'\n",
        "first_test_date = final_df[final_df.index >= split_date].index.min()\n",
        "split_date = first_test_date\n",
        "\n",
        "X_train = X[X.index < split_date]\n",
        "X_test = X[X.index >= split_date]\n",
        "y_train = y[y.index < split_date]\n",
        "y_test = y[y.index >= split_date]\n",
        "\n",
        "print(f\"\\nTraining on {len(X_train)} samples (before {split_date.date()}).\")\n",
        "print(f\"Testing on {len(X_test)} samples (on and after {split_date.date()}).\")\n",
        "\n",
        "# --- Train the XGBoost Model with Tuned Hyperparameters ---\n",
        "# We are making the model more conservative to handle noisy financial data\n",
        "model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=2000,      # Increased estimators to allow for smaller learning rate\n",
        "    learning_rate=0.01,     # <-- TUNED: Made smaller to learn more cautiously\n",
        "    max_depth=3,            # <-- TUNED: Made shallower to prevent overfitting to noise\n",
        "    subsample=0.7,\n",
        "    colsample_bytree=0.7,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    early_stopping_rounds=100 # Increased patience for the slower learning rate\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          eval_set=[(X_test, y_test)],\n",
        "          verbose=False)\n",
        "\n",
        "print(\"\\nModel training complete with tuned hyperparameters.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1swEs_SC3NWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUATION"
      ],
      "metadata": {
        "id": "YUUCrBJrJ5nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error by comparing the test set answers (y_test)\n",
        "# with the model's predictions (y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred) # CORRECTED LINE\n",
        "\n",
        "print(f\"\\nModel Performance on Test Set:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {np.sqrt(mse):.6f}\")\n",
        "\n",
        "# Plot predictions vs actuals from the test set\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(y_test.index, y_test, label='Actual Future Returns', color='blue', alpha=0.7)\n",
        "plt.plot(y_test.index, y_pred, label='Predicted Future Returns', color='red', linestyle='--')\n",
        "plt.title('Model Predictions vs. Actual Values')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('5-Day Return')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EGY3fjyAJhCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ADVANCED EVALUATION METRICS\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "\n",
        "# Create a new DataFrame to analyze results\n",
        "results_df = pd.DataFrame({\n",
        "    'Actual': y_test,\n",
        "    'Predicted': y_pred\n",
        "})\n",
        "\n",
        "# Determine the actual and predicted direction (1 for up, 0 for down)\n",
        "results_df['Actual_Direction'] = np.where(results_df['Actual'] > 0, 1, 0)\n",
        "results_df['Predicted_Direction'] = np.where(results_df['Predicted'] > 0, 1, 0)\n",
        "\n",
        "# Calculate the directional accuracy\n",
        "directional_accuracy = np.mean(results_df['Actual_Direction'] == results_df['Predicted_Direction'])\n",
        "\n",
        "print(f\"--- Model Performance Metrics ---\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {np.sqrt(mse):.6f}\")\n",
        "print(f\"Directional Accuracy: {directional_accuracy:.2%}\")\n",
        "\n",
        "# See the first few predictions and their directions\n",
        "print(\"\\n--- Sample of Predictions vs Actuals ---\")\n",
        "print(results_df.head())"
      ],
      "metadata": {
        "id": "xVdQT4CF5W39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dominicftsmith/kaggle-market-prediction.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FbMFZSUkvb9",
        "outputId": "3e572d36-dcce-4efc-b4a3-60c80c6ed376"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kaggle-market-prediction'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 30 (delta 12), reused 10 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (30/30), 661.83 KiB | 3.15 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQOxkngOnqne"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}