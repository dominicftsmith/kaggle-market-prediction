{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Market Beater\n",
        "#### Kaggle's Hull Tactical - Market Prediction Competition\n",
        "---\n",
        "\n",
        "**Predicting Market Predictablity**\n",
        "\n",
        "**By:** Dominic Smith (for I Understand AI)"
      ],
      "metadata": {
        "id": "m_iMHAaGpAmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "bLemCYwAcx5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LOAD DATA"
      ],
      "metadata": {
        "id": "Y4hHa5ckTGDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# Defining path to Google Drive data\n",
        "FILE_PATH = '/content/drive/MyDrive/Kaggle Beat S&P'\n",
        "\n",
        "# --- Loading S&P 500 Daily Price Data ---\n",
        "try:\n",
        "    sp500_df = pd.read_csv(FILE_PATH + '/SP500_daily.csv')\n",
        "    print(\"S&P 500 data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"S&P 500 file not found. Creating a placeholder DataFrame for demonstration.\")\n",
        "    date_range = pd.date_range(start='2015-01-01', end='2025-01-01', freq='B') # Business days\n",
        "    placeholder_prices = 1500 + np.random.randn(len(date_range)).cumsum() * 5\n",
        "    sp500_df = pd.DataFrame({'Date': date_range, 'Close': placeholder_prices})\n",
        "    # Simulate holiday gaps\n",
        "    holidays = np.random.choice(sp500_df.index, 20, replace=False)\n",
        "    sp500_df.loc[holidays, 'Close'] = np.nan\n",
        "    print(\"Placeholder data created with simulated holiday gaps.\")\n",
        "\n",
        "# --- Load Politician Trading Data ---\n",
        "pol_df = pd.read_csv(FILE_PATH + '/insider1.csv')\n",
        "print(\"Politician trading data loaded successfully.\")"
      ],
      "metadata": {
        "id": "jzxl7SjnExTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA PREPROCESSING AND CLEANING"
      ],
      "metadata": {
        "id": "rdLi22e3S1b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load S&P 500 Price Data ---\n",
        "\n",
        "try:\n",
        "    # Correctly load 'SP500_daily.csv' file\n",
        "    # and tell pandas to treat 'observation_date' as a date while loading\n",
        "    sp500_df = pd.read_csv(\n",
        "        FILE_PATH + '/SP500_daily.csv',\n",
        "        parse_dates=['observation_date']\n",
        "    )\n",
        "\n",
        "    # Rename the columns to 'Date' and 'Close' so all the later code works\n",
        "    sp500_df = sp500_df.rename(columns={\n",
        "        'observation_date': 'Date',\n",
        "        'SP500': 'Close'\n",
        "    })\n",
        "\n",
        "    print(\"Successfully loaded SP500_daily.csv and renamed columns.\")\n",
        "    print(\"DataFrame head:\")\n",
        "    print(sp500_df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Could not find 'SP500_daily.csv' in your Google Drive folder.\")\n",
        "    print(\"Please double-check the filename and folder location.\")\n",
        "    raise"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dcuyGqgSruRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8ccd4f-bff5-41d0-d3f5-2b18547e2800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded SP500_daily.csv and renamed columns.\n",
            "DataFrame head:\n",
            "        Date    Close\n",
            "0 2015-09-21  1966.97\n",
            "1 2015-09-22  1942.74\n",
            "2 2015-09-23  1938.76\n",
            "3 2015-09-24  1932.24\n",
            "4 2015-09-25  1931.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. LOAD AND CLEAN POLITICIAN DATA (Revised with Explicit Type Conversion) ---\n",
        "print(\"\\n--- Step 2: Loading and Cleaning Politician Data ---\")\n",
        "try:\n",
        "    pol_df = pd.read_csv(FILE_PATH + '/insider1.csv')\n",
        "\n",
        "    # --- Convert 'Traded' column to datetime ---\n",
        "    # Clean the string first, then convert.\n",
        "    pol_df['Traded'] = pd.to_datetime(pol_df['Traded'].str.replace('\\n', ' '), format='mixed', errors='coerce')\n",
        "\n",
        "    # Add this line to clean the 'Published' date column\n",
        "    pol_df['Published'] = pd.to_datetime(pol_df['Published'].str.replace('\\n', ' '), format='mixed', errors='coerce')\n",
        "\n",
        "    # --- Convert 'Price' column to numeric ---\n",
        "    # The 'Price' column may have '$' signs. This will remove them and convert to a number.\n",
        "    # 'coerce' will turn any errors (like 'N/A' text) into NaN (Not a Number).\n",
        "    pol_df['Price'] = pd.to_numeric(pol_df['Price'].astype(str).str.replace('$', ''), errors='coerce')\n",
        "\n",
        "    # --- Convert 'Size' column to a numeric estimate ---\n",
        "    # This function handles ranges like '1K–15K'.\n",
        "    def size_to_numeric(size_range):\n",
        "        if isinstance(size_range, str):\n",
        "            size_range = size_range.replace('K', '000').replace('M', '000000').replace('$', '')\n",
        "            parts = size_range.split('–')\n",
        "            if len(parts) == 2:\n",
        "                try: return (float(parts[0]) + float(parts[1])) / 2\n",
        "                except ValueError: return np.nan\n",
        "        return np.nan\n",
        "\n",
        "    pol_df['EstimatedValue'] = pol_df['Size'].apply(size_to_numeric)\n",
        "\n",
        "    # Drop rows that couldn't be converted properly\n",
        "    pol_df = pol_df.dropna(subset=['Traded', 'EstimatedValue', 'Price'])\n",
        "\n",
        "    print(\"Politician data loaded and all relevant columns converted successfully.\")\n",
        "    print(\"\\nCleaned pol_df dtypes:\")\n",
        "    print(pol_df[['Traded', 'Price', 'EstimatedValue']].info())\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Could not find the insider1.csv file.\") # More specific error message\n",
        "    raise"
      ],
      "metadata": {
        "id": "iGfaaig-oaS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Process S&P 500 Data ---\n",
        "# The 'observation_date' column is already the datetime index, so no need to convert or set index.\n",
        "\n",
        "# Use forward-fill to handle holidays that might be present in the original data\n",
        "sp500_df['Close'] = sp500_df['Close'].ffill()\n",
        "print(\"\\nHandled missing holiday data in original S&P 500 data using forward-fill.\")\n",
        "\n",
        "# --- Process Politician Trading Data ---\n",
        "# Convert 'Traded' date column to datetime, coercing errors to NaT\n",
        "pol_df['Traded'] = pd.to_datetime(pol_df['Traded'], errors='coerce')\n",
        "\n",
        "# Function to convert trade size ranges (e.g., '1K–15K') to a numeric estimate\n",
        "def size_to_numeric(size_range):\n",
        "    if isinstance(size_range, str):\n",
        "        size_range = size_range.replace('K', '000').replace('M', '000000').replace('$', '')\n",
        "        parts = size_range.split('–')\n",
        "        if len(parts) == 2:\n",
        "            try:\n",
        "                return (float(parts[0]) + float(parts[1])) / 2\n",
        "            except ValueError:\n",
        "                return np.nan\n",
        "    return np.nan\n",
        "\n",
        "pol_df['EstimatedValue'] = pol_df['Size'].apply(size_to_numeric)\n",
        "pol_df = pol_df.dropna(subset=['EstimatedValue'])"
      ],
      "metadata": {
        "id": "fH3cvdxiSwt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "NHgOS9z10fg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with our clean S&P 500 data as the base\n",
        "main_df = sp500_df.copy()\n",
        "\n",
        "# Set the 'Date' column as the index and drop the original 'Date' column\n",
        "main_df = main_df.set_index('Date')\n",
        "\n",
        "\n",
        "# --- 1. Price-Based Features ---\n",
        "# These are calculated first, using only the S&P 500 data\n",
        "main_df['Volatility_20D'] = main_df['Close'].rolling(window=20).std()\n",
        "main_df['Momentum_20D'] = main_df['Close'] - main_df['Close'].shift(20)\n",
        "main_df['SMA_50'] = main_df['Close'].rolling(window=50).mean()\n",
        "\n",
        "# --- 2. Political Trading Features ---\n",
        "# Create a daily summary of political trades\n",
        "# Ensure the index is a datetime type for proper merging\n",
        "daily_trades = pol_df.groupby(pol_df['Traded'].dt.date).agg(\n",
        "    buy_count=('Type', lambda x: (x == 'buy').sum()),\n",
        "    sell_count=('Type', lambda x: (x == 'sell').sum()),\n",
        "    buy_volume=('EstimatedValue', lambda x: x[pol_df['Type'] == 'buy'].sum()),\n",
        "    sell_volume=('EstimatedValue', lambda x: x[pol_df['Type'] == 'sell'].sum())\n",
        ")\n",
        "daily_trades.index = pd.to_datetime(daily_trades.index) # Convert index to datetime\n",
        "\n",
        "# Calculate net metrics\n",
        "daily_trades['net_trades'] = daily_trades['buy_count'] - daily_trades['sell_count']\n",
        "daily_trades['net_volume'] = daily_trades['buy_volume'] - daily_trades['sell_volume']\n",
        "\n",
        "# --- 3. Merge Datasets ---\n",
        "# Merge the political features into our main DataFrame.\n",
        "# Use a 'left' merge to keep all the S&P 500 dates.\n",
        "# Days with no political trades will have NaN values.\n",
        "main_df = main_df.merge(\n",
        "    daily_trades[['net_trades', 'net_volume']],\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill the NaN values for days with no trades with 0\n",
        "main_df[['net_trades', 'net_volume']] = main_df[['net_trades', 'net_volume']].fillna(0)\n",
        "\n",
        "# Reindex to a full daily calendar to ensure all weekdays are present *after* merging\n",
        "# This step introduced NaT values, removing it.\n",
        "# main_df = main_df.asfreq('D')\n",
        "\n",
        "# Use forward-fill again to handle weekends and holidays introduced by asfreq\n",
        "# main_df['Close'] = main_df['Close'].ffill()\n",
        "\n",
        "\n",
        "# --- 4. Define the Target Variable ---\n",
        "# The target is the 5-day future return. This must be the LAST step.\n",
        "main_df['target'] = (main_df['Close'].shift(-5) - main_df['Close']) / main_df['Close']\n",
        "\n",
        "print(\"Feature engineering and merge complete.\")\n",
        "print(\"DataFrame tail (showing recent data with political trades):\")\n",
        "print(main_df.tail(10))"
      ],
      "metadata": {
        "id": "todZGbTn0dG7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(main_df.head())\n",
        "print(main_df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Tru4WEnOr3FD",
        "outputId": "a8a1742c-979a-4ae1-e8b6-89fa0fde74af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date  Close  Volatility_20D  Momentum_20D  SMA_50  net_trades  \\\n",
            "1970-01-01  NaT    NaN             NaN           NaN     NaN         NaN   \n",
            "\n",
            "            net_volume  target  \n",
            "1970-01-01         NaN     NaN  \n",
            "Date              datetime64[ns]\n",
            "Close                    float64\n",
            "Volatility_20D           float64\n",
            "Momentum_20D             float64\n",
            "SMA_50                   float64\n",
            "net_trades               float64\n",
            "net_volume               float64\n",
            "target                   float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MODEL TRAINING"
      ],
      "metadata": {
        "id": "Z4iE-TXy3KCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for XGBoost\n",
        "\n",
        "# Drop rows with NaN values from the entire dataframe *before* splitting\n",
        "main_df_cleaned = main_df.dropna()\n",
        "\n",
        "# Define features (X) and target (y) using the cleaned dataframe\n",
        "features = ['Volatility_20D', 'Momentum_20D', 'SMA_50', 'net_trades', 'net_volume']\n",
        "X = main_df_cleaned[features]\n",
        "y = main_df_cleaned['target']\n",
        "\n",
        "# --- Split Data Using a Specific Date ---\n",
        "# We will train on data before the political trades begin,\n",
        "# and test on data from that point forward.\n",
        "\n",
        "# Find the first valid date in your data on or after March 14, 2023\n",
        "first_test_date = main_df_cleaned[main_df_cleaned.index >= '2023-03-14'].index.min()\n",
        "split_date = first_test_date\n",
        "\n",
        "print(f\"Adjusted split date to first available date: {split_date.date()}\")\n",
        "\n",
        "\n",
        "X_train = X[X.index < split_date]\n",
        "X_test = X[X.index >= split_date]\n",
        "\n",
        "y_train = y[y.index < split_date]\n",
        "y_test = y[y.index >= split_date]\n",
        "\n",
        "\n",
        "print(f\"\\nTraining on {len(X_train)} samples, testing on {len(X_test)} samples.\")\n",
        "\n",
        "# Initialize and train the XGBoost Regressor model\n",
        "model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          eval_set=[(X_test, y_test)])\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1swEs_SC3NWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUATION"
      ],
      "metadata": {
        "id": "YUUCrBJrJ5nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error by comparing the test set answers (y_test)\n",
        "# with the model's predictions (y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred) # CORRECTED LINE\n",
        "\n",
        "print(f\"\\nModel Performance on Test Set:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {np.sqrt(mse):.6f}\")\n",
        "\n",
        "# Plot predictions vs actuals from the test set\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(y_test.index, y_test, label='Actual Future Returns', color='blue', alpha=0.7)\n",
        "plt.plot(y_test.index, y_pred, label='Predicted Future Returns', color='red', linestyle='--')\n",
        "plt.title('Model Predictions vs. Actual Values')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('5-Day Return')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EGY3fjyAJhCQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}