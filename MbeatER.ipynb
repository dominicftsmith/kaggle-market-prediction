{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Market Beater\n",
        "#### Kaggle's Hull Tactical - Market Prediction Competition\n",
        "---\n",
        "\n",
        "**Predicting Market Predictablity**\n",
        "\n",
        "**By:** Dominic Smith (for I Understand AI)"
      ],
      "metadata": {
        "id": "m_iMHAaGpAmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "bLemCYwAcx5P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LOAD DATA"
      ],
      "metadata": {
        "id": "Y4hHa5ckTGDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# Defining path to Google Drive data\n",
        "FILE_PATH = '/content/drive/MyDrive/Kaggle Beat S&P'\n",
        "\n",
        "# --- Loading S&P 500 Daily Price Data ---\n",
        "try:\n",
        "    sp500_df = pd.read_csv(FILE_PATH + '/SP500_daily.csv')\n",
        "    print(\"S&P 500 data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"S&P 500 file not found. Creating a placeholder DataFrame for demonstration.\")\n",
        "    date_range = pd.date_range(start='2015-01-01', end='2025-01-01', freq='B') # Business days\n",
        "    placeholder_prices = 1500 + np.random.randn(len(date_range)).cumsum() * 5\n",
        "    sp500_df = pd.DataFrame({'Date': date_range, 'Close': placeholder_prices})\n",
        "    # Simulate holiday gaps\n",
        "    holidays = np.random.choice(sp500_df.index, 20, replace=False)\n",
        "    sp500_df.loc[holidays, 'Close'] = np.nan\n",
        "    print(\"Placeholder data created with simulated holiday gaps.\")\n",
        "\n",
        "# --- Load Politician Trading Data ---\n",
        "pol_df = pd.read_csv(FILE_PATH + '/insider1.csv')\n",
        "print(\"Politician trading data loaded successfully.\")"
      ],
      "metadata": {
        "id": "jzxl7SjnExTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA PREPROCESSING AND CLEANING"
      ],
      "metadata": {
        "id": "rdLi22e3S1b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load S&P 500 Price Data ---\n",
        "\n",
        "try:\n",
        "    # Correctly load 'SP500_daily.csv' file\n",
        "    # and tell pandas to treat 'observation_date' as a date while loading\n",
        "    sp500_df = pd.read_csv(\n",
        "        FILE_PATH + '/SP500_daily.csv',\n",
        "        parse_dates=['observation_date']\n",
        "    )\n",
        "\n",
        "    # Rename the columns to 'Date' and 'Close' so all the later code works\n",
        "    sp500_df = sp500_df.rename(columns={\n",
        "        'observation_date': 'Date',\n",
        "        'SP500': 'Close'\n",
        "    })\n",
        "\n",
        "    print(\"Successfully loaded SP500_daily.csv and renamed columns.\")\n",
        "    print(\"DataFrame head:\")\n",
        "    print(sp500_df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Could not find 'SP500_daily.csv' in your Google Drive folder.\")\n",
        "    print(\"Please double-check the filename and folder location.\")\n",
        "    raise"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dcuyGqgSruRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Process S&P 500 Data ---\n",
        "# The 'observation_date' column is already the datetime index, so no need to convert or set index.\n",
        "\n",
        "# Use forward-fill to handle holidays that might be present in the original data\n",
        "sp500_df['Close'] = sp500_df['Close'].ffill()\n",
        "print(\"\\nHandled missing holiday data in original S&P 500 data using forward-fill.\")\n",
        "\n",
        "# --- Process Politician Trading Data ---\n",
        "# Convert 'Traded' date column to datetime, coercing errors to NaT\n",
        "pol_df['Traded'] = pd.to_datetime(pol_df['Traded'], errors='coerce')\n",
        "\n",
        "# Function to convert trade size ranges (e.g., '1K–15K') to a numeric estimate\n",
        "def size_to_numeric(size_range):\n",
        "    if isinstance(size_range, str):\n",
        "        size_range = size_range.replace('K', '000').replace('M', '000000').replace('$', '')\n",
        "        parts = size_range.split('–')\n",
        "        if len(parts) == 2:\n",
        "            try:\n",
        "                return (float(parts[0]) + float(parts[1])) / 2\n",
        "            except ValueError:\n",
        "                return np.nan\n",
        "    return np.nan\n",
        "\n",
        "pol_df['EstimatedValue'] = pol_df['Size'].apply(size_to_numeric)\n",
        "pol_df = pol_df.dropna(subset=['EstimatedValue'])"
      ],
      "metadata": {
        "id": "fH3cvdxiSwt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "NHgOS9z10fg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with our clean S&P 500 data as the base\n",
        "main_df = sp500_df.copy()\n",
        "\n",
        "# --- 1. Price-Based Features ---\n",
        "# These are calculated first, using only the S&P 500 data\n",
        "main_df['Volatility_20D'] = main_df['Close'].rolling(window=20).std()\n",
        "main_df['Momentum_20D'] = main_df['Close'] - main_df['Close'].shift(20)\n",
        "main_df['SMA_50'] = main_df['Close'].rolling(window=50).mean()\n",
        "\n",
        "# --- 2. Political Trading Features ---\n",
        "# Create a daily summary of political trades\n",
        "# Ensure the index is a datetime type for proper merging\n",
        "daily_trades = pol_df.groupby(pol_df['Traded'].dt.date).agg(\n",
        "    buy_count=('Type', lambda x: (x == 'buy').sum()),\n",
        "    sell_count=('Type', lambda x: (x == 'sell').sum()),\n",
        "    buy_volume=('EstimatedValue', lambda x: x[pol_df['Type'] == 'buy'].sum()),\n",
        "    sell_volume=('EstimatedValue', lambda x: x[pol_df['Type'] == 'sell'].sum())\n",
        ")\n",
        "daily_trades.index = pd.to_datetime(daily_trades.index) # Convert index to datetime\n",
        "\n",
        "# Calculate net metrics\n",
        "daily_trades['net_trades'] = daily_trades['buy_count'] - daily_trades['sell_count']\n",
        "daily_trades['net_volume'] = daily_trades['buy_volume'] - daily_trades['sell_volume']\n",
        "\n",
        "# --- 3. Merge Datasets ---\n",
        "# Merge the political features into our main DataFrame.\n",
        "# Use a 'left' merge to keep all the S&P 500 dates.\n",
        "# Days with no political trades will have NaN values.\n",
        "main_df = main_df.merge(\n",
        "    daily_trades[['net_trades', 'net_volume']],\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill the NaN values for days with no trades with 0\n",
        "main_df[['net_trades', 'net_volume']] = main_df[['net_trades', 'net_volume']].fillna(0)\n",
        "\n",
        "# Reindex to a full daily calendar to ensure all weekdays are present *after* merging\n",
        "main_df = main_df.asfreq('D')\n",
        "\n",
        "# Use forward-fill again to handle weekends and holidays introduced by asfreq\n",
        "main_df['Close'] = main_df['Close'].ffill()\n",
        "\n",
        "\n",
        "# --- 4. Define the Target Variable ---\n",
        "# The target is the 5-day future return. This must be the LAST step.\n",
        "main_df['target'] = (main_df['Close'].shift(-5) - main_df['Close']) / main_df['Close']\n",
        "\n",
        "print(\"Feature engineering and merge complete.\")\n",
        "print(\"DataFrame tail (showing recent data with political trades):\")\n",
        "print(main_df.tail(10))"
      ],
      "metadata": {
        "id": "todZGbTn0dG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MODEL TRAINING"
      ],
      "metadata": {
        "id": "Z4iE-TXy3KCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for XGBoost\n",
        "\n",
        "# Drop rows with NaN values from the entire dataframe *before* splitting\n",
        "main_df_cleaned = main_df.dropna()\n",
        "\n",
        "# Define features (X) and target (y) using the cleaned dataframe\n",
        "features = ['Volatility_20D', 'Momentum_20D', 'SMA_50', 'net_trades', 'net_volume']\n",
        "X = main_df_cleaned[features]\n",
        "y = main_df_cleaned['target']\n",
        "\n",
        "# Split data chronologically for time-series validation\n",
        "# Use a fixed date for splitting to ensure test set is not empty\n",
        "split_date = '2023-01-01' # Example split date, adjust as needed\n",
        "split_index = X.index.get_loc(split_date)\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "print(f\"\\nTraining on {len(X_train)} samples, testing on {len(X_test)} samples.\")\n",
        "\n",
        "# Initialize and train the XGBoost Regressor model\n",
        "model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          eval_set=[(X_test, y_test)])\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1swEs_SC3NWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUATION"
      ],
      "metadata": {
        "id": "YUUCrBJrJ5nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error by comparing the test set answers (y_test)\n",
        "# with the model's predictions (y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred) # CORRECTED LINE\n",
        "\n",
        "print(f\"\\nModel Performance on Test Set:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {np.sqrt(mse):.6f}\")\n",
        "\n",
        "# Plot predictions vs actuals from the test set\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(y_test.index, y_test, label='Actual Future Returns', color='blue', alpha=0.7)\n",
        "plt.plot(y_test.index, y_pred, label='Predicted Future Returns', color='red', linestyle='--')\n",
        "plt.title('Model Predictions vs. Actual Values')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('5-Day Return')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EGY3fjyAJhCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The '!' tells Colab to run this as a shell command\n",
        "!git clone https://github.com/dominicftsmith/kaggle-market-prediction.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiwqONVEFMrT",
        "outputId": "d64d446a-cbf3-4f32-8c9c-67e7b2806aae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kaggle-market-prediction'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (5/5), done.\n"
          ]
        }
      ]
    }
  ]
}